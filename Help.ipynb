{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb40cf6-a945-4521-8e29-57a6b64ce046",
   "metadata": {},
   "source": [
    "## Secrets & Authentication\n",
    "\n",
    "At the Home page, Users are promted by a form to input credentials to access Source Metadata and Articles tables in our Postgres database.\n",
    "\n",
    "Credentials are written to their respective files\n",
    "\n",
    "1. ``.secrets_source_metadata``\n",
    "2. ``.secrets_articles``\n",
    "\n",
    "Upon submitting the form, we make sure that credentials are not blank and that they work. To test that credentials are correct, we call ``test_db_credentials()`` and pass which database a user wants to test to the function (Source Metadata or Articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b73a927-f47d-45d1-af34-da0169dac967",
   "metadata": {},
   "outputs": [],
   "source": [
    "form = st.form(key=\"db\")\n",
    "database_selector = form.radio(\"Which database would you like to add your credentials to?\", options=[\"Source Metadata\", \"Articles\"])\n",
    "host = form.text_input(label=\"HOST\")\n",
    "port = form.text_input(label=\"PORT\")\n",
    "user_name = form.text_input(label=\"USERNAME\")\n",
    "password = form.text_input(label=\"PASSWORD\", type=\"password\")\n",
    "submit_button = form.form_submit_button(label=\"Submit\")\n",
    "fields = [host, port, user_name, password]\n",
    "\n",
    "if submit_button:\n",
    "    if any(field == \"\" for field in fields):\n",
    "        st.error(\"❌ Missing field!\")\n",
    "    else:\n",
    "        if database_selector == \"Source Metadata\":\n",
    "            with open(\".secrets_source_metadata\", \"w\") as secrets:\n",
    "                secrets.write(\n",
    "                        f'HOST=\"{host}\"\\nPORT=\"{port}\"\\nPASSWORD=\"{password}\"\\nUSERNAME=\"{user_name}\"\\nDATABASE=\"metadata\"'\n",
    "                )\n",
    "                secrets.close()\n",
    "                test_db_credentials(database_selector)\n",
    "        if database_selector == \"Articles\":\n",
    "            with open(\".secrets_articles\", \"w\") as secrets:\n",
    "                secrets.write(\n",
    "                        f'HOST=\"{host}\"\\nPORT=\"{port}\"\\nPASSWORD=\"{password}\"\\nUSERNAME=\"{user_name}\"\\nDATABASE=\"article_storage\"'\n",
    "                )\n",
    "                secrets.close()\n",
    "                test_db_credentials(database_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2ecb9-6508-46d0-8915-3a9a01ebe69e",
   "metadata": {},
   "source": [
    "The test_db_credentials function lives in our db module. It simply tries to connect and registers success/failure of the connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca871e0b-7f3f-4af5-8b8b-73d8c57b38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_db_credentials(db_name) -> None:\n",
    "    \"\"\"Try to connect to db. Toasts success/warning if credentials provided are correct/incorrect.\"\"\" \n",
    "    secrets: dict = select_db(db_name)\n",
    "    st.toast(\"Checking credentials\")\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            database = secrets['DATABASE'], \n",
    "            user = secrets['USERNAME'], \n",
    "            host = secrets['HOST'], \n",
    "            password = secrets['PASSWORD'], \n",
    "            port = secrets['PORT']\n",
    "        )\n",
    "        st.toast(\"✅ VALID CREDENTIALS\")\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        st.toast(f\"ENSURE CREDENTIALS ARE VALID AND THAT VPN IS ON\", icon=\"🚨\")\n",
    "        st.exception(e)\n",
    "        st.warning(\"Have you checked your VPN and credentials?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921f766-a6e5-4732-9388-14c11e20374e",
   "metadata": {},
   "source": [
    "## Country Reviews\n",
    "\n",
    "### Session state\n",
    "\n",
    "When a user enters the country review page, they are met with the option to upload or create a workbook. For the user to manipulate the workbook in session, we save the file to session state.\n",
    "\n",
    "> If there's an error with the workbook it is likely due to cache. Clear cache and refresh your page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbb3c6a-48bd-4557-b1e4-57ee9371e116",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_workbook' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialise workbook in session memory\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworkbook\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m st\u001b[38;5;241m.\u001b[39msession_state:\n\u001b[0;32m----> 3\u001b[0m     st\u001b[38;5;241m.\u001b[39msession_state\u001b[38;5;241m.\u001b[39mworkbook \u001b[38;5;241m=\u001b[39m \u001b[43minit_workbook\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_workbook' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialise workbook in session memory\n",
    "if \"workbook\" not in st.session_state:\n",
    "    st.session_state.workbook = init_workbook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fd00f-1bb0-446d-a7bc-c6dfca1c75c0",
   "metadata": {},
   "source": [
    "\n",
    "### Countries\n",
    "\n",
    "Users need to select a country they want to query. The country selected will be the name of their workbook ie ``\"AM in xxx\"``. The country selected is also the code sent to query the database.\n",
    "\n",
    "We use ``pycountry.countries`` to get the name and iso-2 code of the country that user wants to investigate.\n",
    "\n",
    "> If there's an error in fetching data it might be because we don't have the country in our db or we are not using the iso-2 code (this scenario is only likely if the country is disputed like Kosovo.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d51bc5-cefb-4b3f-99bb-47341ce92d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract name from pycountry\n",
    "countries = [i.name for i in pycountry.countries]\n",
    "with st.form(\"init_research\"):\n",
    "    country_selectbox = st.selectbox(label=\"Select country to research\", options=countries, index=None, placeholder=\"Select Country for Review\")\n",
    "    uploaded_file = st.file_uploader(\"Upload existing research sheet\", type=['xlsx'])\n",
    "    st.subheader(\"Create or upload workbook\")\n",
    "    create_worksheet_btn = st.form_submit_button(\"Create workbook\", use_container_width=True)\n",
    "\n",
    "if create_worksheet_btn:\n",
    "\n",
    "    # If user is not uploading a file and wants to create one, we run init_workbook()\n",
    "    if uploaded_file is None:\n",
    "        if country_selectbox is None:\n",
    "            st.toast(\"Select country\")\n",
    "            st.stop()\n",
    "        ## Create workbook by calling init_workbook from the country_review module and save the workbook as a BytesIO object to current session.\n",
    "        workbook: BytesIO = init_workbook()\n",
    "        st.session_state.workbook = workbook\n",
    "        st.toast(\"Workbook created\")\n",
    "\n",
    "    #If User has uploaded a file, pass the file to init_workbook() and save the BytesIO object to session state\n",
    "    else:\n",
    "            st.session_state.workbook = init_workbook(uploaded_file)\n",
    "            st.toast(\"Workbook uploaded successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc6bb5-a5ce-4bbf-8629-4fb72a9429c5",
   "metadata": {},
   "source": [
    "## Create Workbook\n",
    "\n",
    "We create a workbook with the ``init_workbook()`` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d4e1a59-b72a-4742-9d0a-b27dfa4c9479",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BytesIO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_workbook\u001b[39m(file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mBytesIO\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m#If we upload a file read and return the file as BytesIO\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m BytesIO(file\u001b[38;5;241m.\u001b[39mread())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BytesIO' is not defined"
     ]
    }
   ],
   "source": [
    "def init_workbook(file=None) -> BytesIO:\n",
    "    if file:\n",
    "        #If we upload a file read and return the file as BytesIO\n",
    "        return BytesIO(file.read())\n",
    "    else:\n",
    "        # If we need to create a file, create a workbook.\n",
    "        wb = openpyxl.Workbook()\n",
    "\n",
    "        # Opynpyxl always creates a default sheet. We don't want \"Sheet\" because it adds tabs to our workbook and causes empty sheet errors when writing to the file.\n",
    "        # To only have relevant sheets, set Openpyxl's default Sheet as comply so we can overwrite this and avoid empty sheet errors.\n",
    "        for w in wb.worksheets:\n",
    "            w.title = \"comply\"\n",
    "\n",
    "        stream = BytesIO()\n",
    "        wb.save(stream)\n",
    "\n",
    "        stream.seek(0)\n",
    "        return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0b972-5937-4d7e-8700-78cf6ed049a7",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "We're back in the Country Review page. After you create your workbook, you'll want to get relevant data. \n",
    "The Comply Advantage button and the Wikipedia button fetch data on newspapers and domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a676e3-da36-4d65-968f-d6efa39e23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with st.form(\"Comply Advantage\"):\n",
    "    query_comply_btn = st.form_submit_button(label=\"Populate Comply worksheet\")\n",
    "    \n",
    "st.subheader(\"Wikipedia\") \n",
    "with st.form(\"DbPedia\"):\n",
    "    dbPedia_lang = st.radio(\"Language\", [\"English\", \"French\"], horizontal=True)\n",
    "    dbPedia_query = st.text_input(label=\"Name of wikipedia page\")\n",
    "    dbPedia_btn = st.form_submit_button(label=\"Populate Wikipedia worksheet\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aef97b-6aba-430b-bee1-549255e44723",
   "metadata": {},
   "source": [
    "### Comply\n",
    "\n",
    "The Comply button requires that you select the country that you want to review or else an error is thrown.\n",
    "\n",
    "#### Convert Country name to Country Code\n",
    "\n",
    "Our Source Metadata Table lists countries in the ``ISO-2 code`` so our first step is to use the name of the country that the user selected and fetch the relevant ISO to code. Fortunately, the pycountry module comes with a ``get`` method for looking up country codes based on country name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cdaa404-fdb9-4883-ba3a-eb5a1b669cdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pycountry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m country_code \u001b[38;5;241m=\u001b[39m \u001b[43mpycountry\u001b[49m\u001b[38;5;241m.\u001b[39mcountries\u001b[38;5;241m.\u001b[39mget(name\u001b[38;5;241m=\u001b[39mcountry_selectbox)\u001b[38;5;241m.\u001b[39malpha_2 \n",
      "\u001b[0;31mNameError\u001b[0m: name 'pycountry' is not defined"
     ]
    }
   ],
   "source": [
    "country_code = pycountry.countries.get(name=country_selectbox).alpha_2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691bd9f-b34b-4c8f-ba51-52447b324c7b",
   "metadata": {},
   "source": [
    "#### Query Source Metadata Table\n",
    "\n",
    "Now that we have the ISO code that our Source Metadata Table requires in order to filter domains by country, we'll call the ``db_get_country_domains(country_code)`` and pass our ISO-2 code to the function.\n",
    "\n",
    "The result of the query is stored in a pd.DataFrame which is passed to our ``excel_writer`` function which we use to write the result to our workbook.\n",
    "\n",
    "> Query will fail if you provide an invalid country code or fail to connect to the db because you don't have a VPN on or because credentials are not up to date.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c9af8-eacf-4af6-ac8c-d37b6350861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If user queries comply, we convert name of country to iso-2 code, pass code to db query and write results from dataframe to our file.\n",
    "if query_comply_btn:\n",
    "    if country_selectbox is None:\n",
    "        st.toast(\"⚠️ Select Country!\")\n",
    "        st.stop()\n",
    "    country_code = pycountry.countries.get(name=country_selectbox).alpha_2 \n",
    "    sql_response_country: pd.DataFrame = db_get_country_domains(country_code)\n",
    "    st.write(sql_response_country)\n",
    "    st.session_state.workbook = excel_writer(workbook_stream=st.session_state.workbook, sheet_name=\"comply\", df=sql_response_country)\n",
    "    st.toast(\"Successfully loaded comply data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a63e8f-240f-4f7f-8001-0b291e2a9ec3",
   "metadata": {},
   "source": [
    "The ``db_get_country_domains(country_code)`` exists inside the utils module and country_reviews.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941fd90-e475-484d-9294-0d86d73c430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_get_country_domains(country: str) -> list[tuple] | Exception:\n",
    "    conn: Psycopg2Connection | Exception = connect_to_db(db_name=\"Source Metadata\")\n",
    "    if isinstance(conn, Psycopg2Connection):\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(get_db_domains, (country,))\n",
    "            sql_response = cur.fetchall()\n",
    "            colnames = [desc[0] for desc in cur.description]\n",
    "            return pd.DataFrame(sql_response, columns=colnames)\n",
    "        except Exception as e:\n",
    "            st.warning(str(e))\n",
    "            conn.rollback()\n",
    "        finally:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845666d-259f-48d7-ac03-85195125f3dc",
   "metadata": {},
   "source": [
    "The function creates a session by calling the ``connect_to_db`` from our db module (db.py file) and executes the ``get_db_domains`` query inside the queries.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c53b8-0197-45f1-9a92-9f88a0dda052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db(db_name: str) -> Psycopg2Connection | Exception:\n",
    "    secrets = select_db(db_name)\n",
    "    st.toast(\"Connecting to db\")\n",
    "    try:\n",
    "        conn = psycopg2.connect(database = secrets[\"DATABASE\"] , user = secrets['USERNAME'], host = secrets['HOST'], password = secrets['PASSWORD'], port = secrets['PORT'])\n",
    "        return conn\n",
    "\n",
    "    except Exception as e:\n",
    "        st.write(\"⚠️WARNING\", str(e)) \n",
    "        return e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfbaf5e8-3dc5-46cb-8e2a-05018cdbd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_db_domains = f\"\"\"\n",
    "    SELECT * FROM website WHERE country = %s;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71962878-f4cd-400a-a003-c0b82f73ceea",
   "metadata": {},
   "source": [
    "#### Write to Workbook\n",
    "\n",
    "Whenever we fetch data, we pass the data to the ``excel_writer`` function. This function writes dataframes to sheet and requires the name of the worksheet, the dataframe, and the workbook from streamlit session (as BytesIO). \n",
    "\n",
    "After we recieve the data from Source Metadata, we pass the dataframe to the ``excel_writer.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7238f686-a3b4-4cf0-9986-f425bced6332",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BytesIO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexcel_writer\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     workbook_stream: \u001b[43mBytesIO\u001b[49m, sheet_name: \u001b[38;5;28mstr\u001b[39m, df: pd\u001b[38;5;241m.\u001b[39mDataFrame\n\u001b[1;32m      3\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BytesIO:\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#Excel doesn't accept NaN so replacing with None\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwhere(pd\u001b[38;5;241m.\u001b[39mnotnull(df), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     workbook_stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BytesIO' is not defined"
     ]
    }
   ],
   "source": [
    "def excel_writer(\n",
    "    workbook_stream: BytesIO, sheet_name: str, df: pd.DataFrame\n",
    ") -> BytesIO:\n",
    "\n",
    "    #Excel doesn't accept NaN so replacing with None\n",
    "    df = df.where(pd.notnull(df), \"\")\n",
    "\n",
    "    workbook_stream.seek(0)\n",
    "    workbook = openpyxl.load_workbook(workbook_stream)\n",
    "    if sheet_name in workbook.sheetnames:\n",
    "        del workbook[sheet_name]\n",
    "\n",
    "    sheet = workbook.create_sheet(title=sheet_name)\n",
    "\n",
    "    # Write columns\n",
    "    for col_idx, col_name in enumerate(df.columns, 1):\n",
    "        sheet.cell(row=1, column=col_idx, value=col_name)\n",
    "\n",
    "    # Write rows\n",
    "    '''Dealing with ValueError cannot convert[nan] to Excel indicates that there is an issue with converting a list containing nan. Excel doesn't support NaN so we convert [nan, nan, x] to str.'''\n",
    "\n",
    "\n",
    "    for r_idx, row in enumerate(df.itertuples(index=False), 2):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            if isinstance(value, list):\n",
    "                value = ', '.join(map(str, value))\n",
    "            if value is None:\n",
    "                value = ''\n",
    "\n",
    "            sheet.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "    output_stream = BytesIO()\n",
    "    workbook.save(output_stream)\n",
    "    output_stream.seek(0)\n",
    "    return output_stream\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afea8d6-4bd4-4959-81e2-351983cfa552",
   "metadata": {},
   "source": [
    "## Query DBPedia\n",
    "\n",
    "Cartier has a built in DBPedia scraper for scraping the page for a country - this page is usually phrased as ``List of newspaper in xxxx`` where xxxx is the name of the country.\n",
    "\n",
    "Inside the ```mappings`` folder are yaml files for mapping English language and French language DBPedia.\n",
    "\n",
    "The user simply selects the language they want to scrape (each have different endpoints on DBPedia and different data columns/mappings). \n",
    "\n",
    "The relevant language mappings, language, endpoint are passed to the ``get-db_data`` function and the data returned is stored in a pd.DataFrame and passed to excel_writer with the sheet_name set as 'Wikipedia'.\n",
    "\n",
    "> If the DBPedia scraper stops working, make sure that the endpoint and relevant mappings are up to date by querying DBPedia from their website. Update mappings and endpoint if need be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce03f55-ca79-48dc-85ee-3c0c6795ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if dbPedia_btn:\n",
    "    match dbPedia_lang:\n",
    "        case \"English\":\n",
    "            yaml_file = \"en.yml\"\n",
    "            endpoint = \"https://dbpedia.org/sparql\"\n",
    "            lng = \"@en\"\n",
    "        case \"French\":\n",
    "            yaml_file = \"fr.yml\"\n",
    "            lng = \"@fr\"\n",
    "    \n",
    "    dbPedia_response: pd.DataFrame | None = get_db_data(yaml_file, dbPedia_query, lng, endpoint)\n",
    "    if dbPedia_response.empty:\n",
    "        st.stop()\n",
    "    \n",
    "    st.session_state.workbook = excel_writer(workbook_stream=st.session_state.workbook, sheet_name=\"wikipedia\", df=dbPedia_response)\n",
    "    st.dataframe(dbPedia_response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81db6ee-e597-44d3-a1c1-fa848df9cafa",
   "metadata": {},
   "source": [
    "## Collate Sources & Download Workbook\n",
    "\n",
    "### Download Button\n",
    "\n",
    "Now that we have queried Comply data and data from DBPedia and created respective worksheets in our workbook for their data, we'll want to label our sources and create a source of truth.\n",
    "\n",
    "Labrelling requires that you download the file first with the ``Download research sheet`` button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f046e-d369-44fb-b703-a5cf247b8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.download_button(\n",
    "        label=\"Download research sheet\", \n",
    "        data=st.session_state.workbook, \n",
    "        file_name=f\"AM Coverage in {country_selectbox}.xlsx\", \n",
    "        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
    "        use_container_width=True,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693f2e1-a769-48a5-ba1c-a17263da6447",
   "metadata": {},
   "source": [
    "## Collate Sources into a Masterlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd4be4-0527-42f1-bcf1-b3c94f2d59f0",
   "metadata": {},
   "source": [
    "As you can see from the sturcture of this button, it takes the name from the country you selected at the beginning ie country_selectbox. Failing to select your country will lead to the workbook being named AM Coverage in None, but downloading the workbook will still work as will fetching data and writing sheets.\n",
    "\n",
    "> Before you collate, make sure to create provider, typology columns for each worksheet and ensure that every domain has a name in the name field. If there's no name, the collation process will fail.\n",
    "\n",
    "To collate a workbook that's been labelled, simply upload the workbook again, click create workbook button, and then scroll to the bottom where the Create source of truth button is. Pushing this button will collate all worksheets in your workbook (excluding master) into a master list.\n",
    "\n",
    "We collate worksheets by pushing our workbook to the ``create a master_sheet(st.session_state.workbook)`` function. \n",
    "\n",
    "Collation results in either a dataframe or None. If the sheet is empty, the programme will stop. If not, st.session-state.workbook (your workbook) is updated and the master list is written to \"main\" sheet with the excel_writer function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de21ad-80fd-4e06-81c4-0d704652a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "if match_btn:\n",
    "    master_sheet: pd.DataFrame | None = collate_sources(st.session_state.workbook)\n",
    "    if master_sheet.empty:\n",
    "        st.stop()\n",
    "    st.session_state.workbook = excel_writer(workbook_stream=st.session_state.workbook, sheet_name=\"main\", df=master_sheet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231292e-f5d1-45e9-b524-2665119b26b1",
   "metadata": {},
   "source": [
    "> The collation will err if there are any domains missing a value in the ``name`` field. Make sure to add a provider, typology column to your worksheets so that you can tell which domain came from which source in your master list.\n",
    "\n",
    "### Steps in Collation\n",
    "\n",
    "The collation process creates a column for each worksheet that runs fuzzy matching on the names of the domains and then concats the sheets and groups the concated sheet by the best matching name.\n",
    "\n",
    "The collate sources takes your workbook and applies a number of functions to your workbook\n",
    "\n",
    "1. Destructs your BytesIO workbook and loads your workbook as a Workbook object\n",
    "2. Creates a dictionary of sheet_names and corresponding Worksheets (objects in openpyxl) \n",
    "3. Loops through each worksheet and applies normalise_domains(df) to the dataframe in the worksheet.\n",
    "    - Normalise domains is applies regex to remove unwanted text like www from the domains or / at the end.\n",
    "4. The worksheets dictionary with names and dfs are passed to ``match_names function``\n",
    "    - The match_names function loops through keys and worksheets to create a match_name column with the fuzzy matched names.\n",
    "  \n",
    "5. Collate sheets is the final step. This function makes sure that all types are string (this way ciruclation 18,0999 is cast as a string. In Excel and openpyxl, it will be treated as a float by default). Once types are all cast as string, worksheets are concatenated and then grouped by match_name and aggregated into a list with a return. Any failures here are reutrned and flagged with error visible from Cartier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33288f95-a8b9-4f65-acfb-6300c1341532",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This is the main function\n",
    "def collate_sources(workbook: BytesIO) -> pd.DataFrame | None:\n",
    "    workbook.seek(0)\n",
    "    workbook: Workbook = openpyxl.load_workbook(workbook)\n",
    "    sheet_names: list[str] = workbook.sheetnames\n",
    "    worksheet_list: list[Worksheet] = workbook.worksheets\n",
    "    worksheets: dict = read_worksheets(sheet_names, worksheet_list)\n",
    "\n",
    "    # Clean the domains\n",
    "    for df in worksheets.values():\n",
    "        df = normalise_domains(df)\n",
    "    \n",
    "    worksheets: dict = match_names_domains(worksheets)\n",
    "    df: pd.DataFrame | None = collate_sheets(worksheets) \n",
    "    st.toast(\"✔️ Successfully collated sources\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d264d29b-44dd-4101-8c17-97e90fa00efa",
   "metadata": {},
   "source": [
    "#### Breaking down Collate Sources functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aca253-86cd-4b6e-a54c-53d9b95900de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reads the Worksheet objects to dataframes and skips the worksheet called main in case you already have a main worksheet in your workbook. We don't want this to be collated as part of the new master list.\n",
    "def read_worksheets(\n",
    "    sheet_names, work_sheets: list[Worksheet]\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "\n",
    "    data = {}\n",
    "    for name, worksheet in zip(sheet_names, work_sheets):\n",
    "        if name == \"main\":\n",
    "            continue\n",
    "        data[name] = worksheet_to_dataframe(worksheet)\n",
    "\n",
    "    return data\n",
    "\n",
    "## Normalise domains simply applies extract_domain lambda to each domain.\n",
    "def normalise_domains(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    extract_domain = lambda url: re.sub(r\"^(https?://)?(www\\.)?|(\\/)+$\", \"\", url)\n",
    "    if \"domain\" in df.columns:\n",
    "        df[\"domain\"] = df[\"domain\"].apply(\n",
    "            lambda x: extract_domain(x) if x is not None else None\n",
    "        )\n",
    "        return df\n",
    "    return df\n",
    "\n",
    "## Fuzzy matching returns the name that domain name matched with out of all domains in worksheets. Threshold is high at 90 to avoid false positives but can be lower (min 80 is recommended)\n",
    "def fuzzy_matching(value_string: str, match_against: pd.Series, threshold=90) -> str:\n",
    "    match, score, i = process.extractOne(value_string, match_against, scorer=fuzz.ratio)\n",
    "    return match if score >= threshold else value_string\n",
    "\n",
    "## We create a match by applying fuzzy matching to names. There's an option to add our normalised domains to the fuzzy matching by uncommenting domain series.\n",
    "def create_match_col(df: pd.DataFrame, match_against: pd.DataFrame) -> pd.Series | None:\n",
    "    try:\n",
    "        name_series = df[\"name\"].apply(lambda x: fuzzy_matching(x, match_against[\"name\"]))\n",
    "        #domain_series = df[\"domain\"].apply(lambda x: fuzzy_matching(x, match_against[\"domain\"]))\n",
    "        return name_series #, domain_series\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Ensure there are no media sources with a missing name: {e}\")\n",
    "        st.stop()\n",
    "\n",
    "## Match names and domains calls the create_match col function which applies fuzzy_matching function. This funcion updates our dictionary of worksheets.\n",
    "def match_names_domains(worksheets: dict) -> dict | None:\n",
    "    for key, df in worksheets.items():\n",
    "        # if key != 'comply':\n",
    "        name_series: pd.Series = create_match_col(df, worksheets[\"comply\"])\n",
    "        df[\"match_name\"] = name_series\n",
    "        worksheets[key] = df\n",
    "\n",
    "    return worksheets\n",
    "\n",
    "# Collate worksheets concats dataframes (ie our worksheets) and groups them by match_name. This is the final step. The result is a dataframe.\n",
    "def collate_sheets(worksheets: dict) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        # Types in Excel require exact same so circulations that are flaots and str break programme. Cast everything as str\n",
    "        concat_data = pd.concat(worksheets.values()).astype(str)\n",
    "        merged_df = concat_data.groupby(\"match_name\").agg(tuple).applymap(list).reset_index()\n",
    "        return merged_df\n",
    "    except Exception as e:\n",
    "        st.toast(f\"⚠️ Failed to collate {e}\")\n",
    "\n",
    "\n",
    "## Collate sources reads all of the worksheets except main into dataframes and stores them in a dictionary before applying normalising_domains and fuzzy_matching.\n",
    "## Final step is to collate_worksheets by concatenating them and grouping them by fuzzy matched name. Reuslt is a dataframe \n",
    "def collate_sources(workbook: BytesIO) -> pd.DataFrame | None:\n",
    "    workbook.seek(0)\n",
    "    workbook: Workbook = openpyxl.load_workbook(workbook)\n",
    "    sheet_names: list[str] = workbook.sheetnames\n",
    "    worksheet_list: list[Worksheet] = workbook.worksheets\n",
    "    worksheets: dict = read_worksheets(sheet_names, worksheet_list)\n",
    "\n",
    "    # Clean the domains\n",
    "    for df in worksheets.values():\n",
    "        df = normalise_domains(df)\n",
    "    \n",
    "    worksheets: dict = match_names_domains(worksheets)\n",
    "    df: pd.DataFrame | None = collate_sheets(worksheets) \n",
    "    st.toast(\"✔️ Successfully collated sources\")\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "am_streamlit",
   "language": "python",
   "name": "am_streamlit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

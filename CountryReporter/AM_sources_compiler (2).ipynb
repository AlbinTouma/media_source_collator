{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“° AM Source Compiler\n",
        "\n",
        "AM Source compiler merges research sheets based on `source names` and `domain url`. The output is a master list of the sources in the country researched.\n"
      ],
      "metadata": {
        "id": "S2duiufU2YOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SELCT YOUR RESEARCH SHEET\"S URL\n",
        "URL = ''"
      ],
      "metadata": {
        "id": "RMDvNWkai53l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Authenticator\n",
        "\n",
        "AM Source Compiler works with your Google research sheet. Input the URL to your sheet and authenticate yourself."
      ],
      "metadata": {
        "id": "snnEJ8HvdvLu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rim8uuP2Uh2X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "import re\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Authenticator opens in a new window. Give your permisssion to AM Source Compiler to access your sheets."
      ],
      "metadata": {
        "id": "d-BP0S7Nd4n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ],
      "metadata": {
        "id": "wfxcwNfkYriB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Research Sheet"
      ],
      "metadata": {
        "id": "pZsYuyEBgMpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select releveant research doc and load sheets."
      ],
      "metadata": {
        "id": "q7Iu0ao9gHZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sh = gc.open_by_url(URL)"
      ],
      "metadata": {
        "id": "wTghV4Mb3nqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "def open_workbook(workbook):\n",
        "  ws = sh.worksheet(workbook)\n",
        "  rows = ws.get_all_values()\n",
        "  return pd.DataFrame.from_records(rows[1:], columns = rows[0])"
      ],
      "metadata": {
        "id": "mSBWLlS93g4e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the sheets that you want to merge into our master list."
      ],
      "metadata": {
        "id": "Ddln7qQSfHJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get workbooks=\n",
        "comply = open_workbook('comply')\n",
        "worldnews = open_workbook('world_newspapers')\n",
        "wikipedia = open_workbook('wikipedia')\n",
        "press_council = open_workbook(\"press council of ireland\")\n",
        "mediainfo = open_workbook(\"mediainfo\")"
      ],
      "metadata": {
        "id": "BxP9BxHNfFBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare domains\n",
        "\n",
        "URLs come in different formats. To improve matching, we ensure only take what's after 'www.'"
      ],
      "metadata": {
        "id": "az7goa-73PCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean url\n",
        "extract_domain = lambda url: re.sub(r'^(https?://)?(www\\.)?|(\\/)+$', '', url)\n",
        "\n",
        "wikipedia['domain'] = wikipedia['domain'].apply(lambda x: extract_domain(x))\n",
        "comply['domain'] = comply['domain'].apply(lambda x: extract_domain(x))\n",
        "worldnews['domain'] = worldnews['domain'].apply(lambda x: extract_domain(x))\n",
        "mediainfo['domain'] = mediainfo['domain'].apply(lambda x: extract_domain(x))\n",
        "press_council['domain'] = press_council['domain'].apply(lambda x: extract_domain(x))"
      ],
      "metadata": {
        "id": "INY_0VT66avI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge and group into Main list"
      ],
      "metadata": {
        "id": "u0q6jD0dg0Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Concat sheets\n",
        "concat_data = pd.concat([wikipedia, comply, worldnews, mediainfo, press_council]).reset_index()"
      ],
      "metadata": {
        "id": "RQkXd-cXVFF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concat sheets and group by name with aggregated rows as list\n",
        "merged_df = concat_data.groupby(['name', 'domain']).agg(tuple).applymap(list).reset_index()\n",
        "merged_df = concat_data.groupby('name').agg(tuple).applymap(list).reset_index()\n",
        "merged_df = merged_df.sort_values(by='name', ascending=True)"
      ],
      "metadata": {
        "id": "sBZ8mG1Ugyae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Ensure that there's only one unique value in provider and taxonomy fields for easier filtering in sheet. *italicized text*"
      ],
      "metadata": {
        "id": "Cw3qy1DLhFFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['provider'] = merged_df['provider'].apply(lambda x: set(x))\n",
        "merged_df['taxonomy'] = merged_df['taxonomy'].apply(lambda x: set(x))"
      ],
      "metadata": {
        "id": "vXQDVWwAGHt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stringify(lst):\n",
        "  lst = [x for x in lst if not (isinstance(x, float) and np.isnan(x))]\n",
        "  lst = [x for x in lst if x !=\"\"] #Remove where empty string ie if comply does not have taxonomy \"\"\n",
        "  unique = set(lst)\n",
        "  return \", \".join(str(item) for item in unique)\n",
        "\n",
        "\n",
        "merged_df['taxonomy'] = merged_df['taxonomy'].apply(lambda x: stringify(x))\n",
        "merged_df['provider'] = merged_df['provider'].apply(lambda x: stringify(x))\n",
        "merged_df['frequency'] = merged_df['frequency'].apply(lambda x: stringify(x))\n",
        "merged_df['type'] = merged_df['type'].apply(lambda x: stringify(x))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RHN6FX2a9KC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write to file"
      ],
      "metadata": {
        "id": "FJkQJH-Q_cy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'main' in [worksheet.title for worksheet in sh.worksheets()]:\n",
        "  worksheet = sh.worksheet('main')\n",
        "  set_with_dataframe(worksheet, merged_df)\n",
        "else:\n",
        "    sh.add_worksheet(title='main', rows=100, cols=30)\n",
        "    worksheet = sh.worksheet('main')\n",
        "    set_with_dataframe(worksheet, merged_df)\n"
      ],
      "metadata": {
        "id": "RxhGTT6ThILD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}